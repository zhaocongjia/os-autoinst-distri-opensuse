<?xml version="1.0" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>lib/hpcbase.pm</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rel='stylesheet' href='./style.css' />
</head>

<body>



<ul id="index"><li><a href="./index.html"><i>&lt;= Back to file list</i></a></li>
  <li>
    <ul>
      <li><a href="#enable_and_start">enable_and_start</a></li>
      <li><a href="#master_node_names">master_node_names</a></li>
      <li><a href="#slave_node_names">slave_node_names</a></li>
      <li><a href="#cluster_names">cluster_names</a></li>
      <li><a href="#distribute_munge_key">distribute_munge_key</a></li>
      <li><a href="#distribute_slurm_conf">distribute_slurm_conf</a></li>
      <li><a href="#generate_and_distribute_ssh">generate_and_distribute_ssh</a></li>
      <li><a href="#check_nodes_availability">check_nodes_availability</a></li>
      <li><a href="#mount_nfs">mount_nfs</a></li>
      <li><a href="#get_master_ip">get_master_ip</a></li>
      <li><a href="#get_slave_ip">get_slave_ip</a></li>
      <li><a href="#prepare_user_and_group">prepare_user_and_group</a></li>
      <li><a href="#prepare_spack_env">prepare_spack_env</a></li>
      <li><a href="#uninstall_spack_module">uninstall_spack_module</a></li>
      <li><a href="#get_compute_nodes_deps">get_compute_nodes_deps</a></li>
      <li><a href="#setup_nfs_server">setup_nfs_server</a></li>
      <li><a href="#mount_nfs_exports">mount_nfs_exports</a></li>
    </ul>
  </li>
</ul><h1>lib/hpcbase.pm</h1>

<h2 id="enable_and_start">enable_and_start</h2>

<p>Enables and starts given systemd service</p>

<h2 id="master_node_names">master_node_names</h2>

<p>Prepare master node names, so those names could be reused, for instance in config preparation, munge key distribution, etc. The naming follows general pattern of master-slave</p>

<h2 id="slave_node_names">slave_node_names</h2>

<p>Prepare compute node names, so those names could be reused, for instance in config preparation, munge key distribution, etc. The naming follows general pattern of master-slave</p>

<h2 id="cluster_names">cluster_names</h2>

<p>Prepare all node names, so those names could be reused</p>

<h2 id="distribute_munge_key">distribute_munge_key</h2>

<p>Distributes munge keys across all compute nodes of the cluster. This should usually be called from the master node. If a replica master node is expected, key should be also be copied in it too.</p>

<h2 id="distribute_slurm_conf">distribute_slurm_conf</h2>

<p>Distributes slurm config across all compute nodes of the cluster This should usually be called from the master node. If a replica master node is expected, config file should be also be copied in it too.</p>

<h2 id="generate_and_distribute_ssh">generate_and_distribute_ssh</h2>

<pre><code>     generate_and_distribute_ssh($user)</code></pre>

<p>Generates and distributes ssh keys across compute nodes. <code>user</code> by default is set to <b>root</b> user unless another value is passed to the parameters. <code>user</code> is used to determine the user on the remote machine where the ssh_id will be copied. This should usually be called from the master node. If a replica master node is expected, the ssh keys should be also be distributed in it too.</p>

<h2 id="check_nodes_availability">check_nodes_availability</h2>

<p>Checks if all listed HPC cluster nodes are available (ping)</p>

<h2 id="mount_nfs">mount_nfs</h2>

<p>Ensure correct dir is created, and correct NFS dir is mounted on SUT</p>

<h2 id="get_master_ip">get_master_ip</h2>

<p>Check the IP of the master node</p>

<h2 id="get_slave_ip">get_slave_ip</h2>

<p>Check the IP of the slave node</p>

<h2 id="prepare_user_and_group">prepare_user_and_group</h2>

<p>Creating slurm user and group with some pre-defined ID</p>

<h2 id="prepare_spack_env">prepare_spack_env</h2>

<pre><code>  prepare_spack_env($mpi)</code></pre>

<p>After install spack and HPC <code>mpi</code> required packages, prepares env variables. The HPC packages (*-gnu-hpc) use an installation path that is separate from the rest and can be exported via a network file system.</p>

<p>After <code>prepare_spack_env</code> run, <code>spack</code> should be ready to build entire tool stack, downloading and installing all bits required for whatever package or compiler.</p>

<h2 id="uninstall_spack_module">uninstall_spack_module</h2>

<pre><code>  uninstall_spack_module($module)</code></pre>

<p>Unload and uninstall <code>module</code> from spack stack</p>

<h2 id="get_compute_nodes_deps">get_compute_nodes_deps</h2>

<pre><code>  get_compute_nodes_deps($mpi)</code></pre>

<p>This function is used to select dependencies packages which are required to be installed on HPC compute nodes in order to run code against particular <code>mpi</code> implementation. <code>get_compute_nodes_deps</code> returns an array of packages</p>

<h2 id="setup_nfs_server">setup_nfs_server</h2>

<p>Prepare a nfs server on the so called management node of the HPC setup. The management node in a minimal setup should provide the directories of *-gnu-hpc installed libraries and the directory with the binaries.</p>

<p><code>exports</code> takes a hash reference with the paths which NFS should make available to the compute nodes in order to run MPI software.</p>

<h2 id="mount_nfs_exports">mount_nfs_exports</h2>

<p>Make the HPC libraries and the location of the binaries available to the so called compute nodes, from the management one. <code>exports</code> takes a hash reference with the paths which the management node share in order to run the MPI binaries</p>


</body>

</html>


